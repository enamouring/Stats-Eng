# -*- coding: utf-8 -*-
"""Audrey_Final Project Notebook Hahn Stats 24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zPojhpFJADdjY09Hq0oJzgRKpBSGjsA3

# Final Project Notebook - Spring 2024
"""

import matplotlib.pyplot as plt #to generate plots
import imageio #to read and write image data
import torch #for tensor computations
import torchvision #for computer vision tasks
from torchvision import models, transforms #for pretrained models and image transformations
import numpy as np #for numerical operations
from torchvision.models import * #to handle image files
from PIL import Image #import the Image class from the Python Imaging Library (PIL) module
import requests #to make HTTP requests
from torchvision import models #import models from torchvision
from torchsummary import summary #to display model summaries

"""Documentation Block: plot(x) - Display a 2D image plot using the input matrix x.
    
    This function creates a figure and a set of subplots using matplotlib's pyplot submodule.
    It then displays the input data as a grayscale image on a 2D regular raster. The axis lines and labels are turned off
    to so the image can be seen clearly by itself. The figure size is set to 20x20 inches for a large display.
    
    The function is used for visualization of the output of various
    operations, like the application of a filter from a convolutional neural network layer to an image.
    
    Parameters:
    x (np.array): 2D array of pixel data, typically an image represented in grayscale.
    
    Returns:
    None: The function directly displays the figure with the image using plt.show().
"""

def plot(x): #defining a plotting function (generate a 2D image plot using the input matrix x)
    fig, ax = plt.subplots()  #create a figure and a set of subplots
    im = ax.imshow(x, cmap='gray')  #display the data x as an image on a 2D regular raster with a grayscale color map
    ax.axis('off')  #turn off the axis lines and labels
    fig.set_size_inches(20, 20)  #set the figure size to be 20x20 inches
    plt.show()  #display the figure with the image

im = imageio.imread('https://raw.githubusercontent.com/imageio/imageio-binaries/master/images/imageio_banner.png') #use the imageio library to read an image from the specified URL and store it in the variable im

plot(im) #plots the image (im)

net = alexnet(pretrained=True).cuda(0) #load a pre-trained AlexNet model and prepare it for GPU processing

normalize = transforms.Normalize( #define a normalization transform with the mean and standard deviation for each color channel (define the normalization parameters to be used on the input images)
    mean=[0.485, 0.456, 0.406],  #the mean is subtracted from each channel of the image
    std=[0.229, 0.224, 0.225]    #each channel of the image is then divided by these std values
)

preprocess = transforms.Compose([ #compose several transforms together to be applied on the image in sequence
    transforms.Resize(256),       #resize the image to 256 pixels on the shortest side
    transforms.CenterCrop(224),   #crop the image to a 224x224 square at the center
    transforms.ToTensor(),        #convert the image to a PyTorch tensor with values in [0, 1]
    normalize                     #apply the normalization defined above
])

im = imageio.imread('https://www.medicalnewstoday.com/content/images/articles/322/322868/golden-retriever-puppy.jpg') #read an image from the web into an array

plot(im)  #plots the new image (im)

image = Image.fromarray(im) #convert to pil (read an image from a URL and convert it to a grayscale image for processing)

img_tensor = preprocess(image) #apply the preprocessing transformations to the PIL image

img_tensor = img_tensor.unsqueeze_(0) #add a batch dimension to the image tensor

img_tensor.shape #retrieve the shape of the tensor; it should be [1, 3, 224, 224] indicating a batch of 1 image with 3 color channels and 224x224 pixels in height and width

img_variable = torch.tensor(img_tensor).cuda(0) #convert the image tensor to a CUDA tensor

out = net(img_variable) #pass the image tensor to the neural network net to get the prediction

label_index = out.cpu().data.numpy().argmax() #move the network output back to CPU, convert to a numpy array, and find the index of the max probability

label_index #output from previous code, indicating the top prediction index

top_list = np.flip(np.argsort(out.cpu().data.numpy())[0][-10:]) #get indices of the top 10 predictions in descending order

LABELS_URL = 'https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json' #URL to the JSON file containing the labels

labels = {int(key):value for (key, value) in requests.get(LABELS_URL).json().items()} #download the labels and create a dictionary converting string keys to integers

print(labels[label_index]) #print the readable label for the top prediction index (class name corresponding to label_index)

for i in range(10): #iterate over the top 10 predictions
    print(labels[top_list[i]]) #print the labels for the top 10 predictions

net #shows representation of AlexNet's architecture

summary(net, (3, 224, 224)) #display a summary of the AlexNet model showing output shapes and parameter counts for each layer when input is 3-channel 224x224 image

out = net.features[0](img_variable).cpu().detach().numpy() #apply the first convolutional layer of the AlexNet to the image tensor, move the output to CPU, detach it from the current graph, and convert it to a NumPy array

plot(out[0,0,:,:]) #call the plot function to visualize the output of the first filter of the first convolutional layer

plt.plot(np.arange(4096),net.classifier[0:6](net.avgpool(net.features[0:13](img_variable)).flatten()).cpu().detach().numpy()) #plot the activations of the sixth layer of the classifier part of AlexNet after passing through all feature layers and the average pooling
fig = plt.gcf() #get the current figure object for further manipulation
fig.set_size_inches(10, 10) #set the size of the current figure

im = imageio.imread('http://bocasurfcam.com/most_recent_image.php') #read an image from the specified URL into the variable im

plot(im) #plots the new image (im)

"""Documentation Block: load_im(im) - Process an input image for model inference.

    Firstly, the input image array is converted to a PIL image. Then, preprocessing transformations are applied to the
    image, including normalization and resizing. The preprocessed image is then converted into a PyTorch tensor,
    unsqueezed to add a batch dimension, and transferred to GPU for acceleration.

    Parameters:
    im (np.array): Raw image data in array format to be processed.

    Returns:
    torch.Tensor: The processed image as a CUDA tensor ready for model inference.
"""

def load_im(im): #define a function load_im
    image = Image.fromarray(im) #convert to pil (read an image from a URL and convert it to a grayscale image for processing)
    img_tensor = preprocess(image) #apply predefined preprocessing to the image
    img_tensor = img_tensor.unsqueeze_(0) #add a batch dimension to create a batch of size 1
    img_variable = torch.tensor(img_tensor).cuda(0) #convert the tensor to a CUDA tensor to use GPU acceleration
    return img_variable #return the CUDA tensor

out = net(load_im(im)) #use the network net to make a prediction on the processed image

"""Documentation Block: inference(im) - Perform inference on an input image using a pre-trained model.

    This function takes an image array, processes it for model compatibility, and then passes it through the pre-trained
    model to obtain predictions. Then it identifies the most fitting label and the top 10 predictions and it prints them out.

    Parameters:
    im (np.array): Image data array for which inference is to be performed.

    Returns:
    None: Prints the labels corresponding to the top 10 predictions.
"""

def inference(im): #define an inference function (perform inference using a pre-trained model on the input image)
    out = net(load_im(im)) #et the model's predictions for the input image
    label_index = out.cpu().data.numpy().argmax() #find the index of the highest prediction score after moving the output to CPU and converting to numpy
    top_list = np.flip(np.argsort(out.cpu().data.numpy())[0][-10:]) #get the indices of the top 10 prediction scores
    print(labels[label_index]) #print the most likely label of the image
    print('____') #print a separator line
    for i in range(10): #iterate over the top 10 predictions
        print(labels[top_list[i]]) #print the labels corresponding to the top 10 predictions

inference(im) #call the inference function with the image im

"""# Restart Notebook (Disconnect and Delete Runtime) Before Running Next Section

# Custom Data Deck
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install wandb #installs weight and bias library
# !apt-get install poppler-utils #installs utilities for PDF files
# !pip install pdf2image #installs pdf to images library (convert)
# !pip install flashtorch #install PyTorch-based visualizations
# import requests #to make HTTP requests
# from pdf2image import convert_from_path #to convert PDF files to images
# import matplotlib.pyplot as plt #to generate plots
# import numpy as np #for numerical operations
# import torch #for tensor computations
# import requests #to make HTTP requests
# from torchvision import * #for computer vision
# from torchvision.models import * #pretained models in torchvision
# from flashtorch.utils import apply_transforms #imports a utility function from FlashTorch for applying image transformations
# import wandb as wb #weight and bias libary

"""Documentation Block: GPU(data) - Converts the input data into a PyTorch tensor that is compatible with GPU operations and enables gradient computation.
    
    This function transfers the input data to the GPU and converts it into
    a tensor, specifying that gradients should be tracked for this tensor (typically used for neural network training).
    
    Parameters:
    data (array-like): The input data that needs to be converted to a tensor. This could be a list, NumPy array, or a PyTorch tensor already on the CPU.
    
    Returns:
    torch.Tensor: The input data as a PyTorch tensor on the GPU with requires_grad set to True.

Documentation Block: GPU_data(data) - Processes an input data array for model inference using predefined preprocessing steps and moves the tensor to GPU.

    This function takes input data and converts it to a PIL image if it is image data, and then applies a series
    of predefined preprocessing steps which may include resizing, cropping, normalization, etc. If the input data is not an image,
    then transformations are applied to match model input requirements. Then the code ensures the data is in tensor
    form, adds a batch dimension if needed, making it suitable for model input, and converts it to a CUDA tensor for
    GPU acceleration.

    Parameters:
    data (array-like or PIL.Image): Data typically read from an image file, array, or a URL. This could be raw
    image data in array format, a PIL Image object, or any array-like structure that contains model input data.

    Returns:
    torch.Tensor: Preprocessed data as a PyTorch tensor ready for model inference on GPU. If the input is already a
    PyTorch tensor, it is simply moved to the GPU.
"""

def GPU(data): #defines a fucntion GPU (transfers data to a GPU device for accelerated computation)
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=torch.device('cuda')) #this function converts data into a PyTorch tensor that requires gradient computation

def GPU_data(data): #defines a function GPU_data (converts data into a PyTorch tensor formatted for use on a GPU)
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=torch.device('cuda')) #this function converts data into a PyTorch tensor that does not require gradient computation

def plot(x): #define a plotting function (generate a 2D image plot using the input matrix x)
    fig, ax = plt.subplots() #create a new figure and a set of subplots
    im = ax.imshow(x, cmap = 'gray') #displays the image x with a gray colormap
    ax.axis('off') #turn off the axis to not show the scale on the image
    fig.set_size_inches(5, 5) #set the size of the figure to 5x5 inches
    plt.show() #display the figure with the image

"""Documentation Block: get_google_slide(url) - Constructs a URL to export a Google Slides presentation as a PDF.

    This function takes a URL to a Google Slides presentation and constructs a new URL that can be used to export
    the presentation as a PDF. It extracts the presentation ID and page ID from the provided URL and constructs the export
    URL.
    
    Parameters:
    url (str): The URL of the Google Slides presentation.
    
    Returns:
    str: The URL for exporting the presentation as a PDF.

Documentation Block: get_slides(url) - Retrieves images of slides from a Google Slides presentation.

    This function takes a URL pointing to a Google Slides presentation, generates a URL for PDF export using the
    get_google_slide function, makes a GET request to the export URL, saves the PDF content to a file, converts the
    PDF file to images at 500 DPI, and returns the list of images representing the slides.
    
    Parameters:
    url (str): The URL of the Google Slides presentation.
    
    Returns:
    list: A list of images representing the slides extracted from the presentation.

Documentation Block: load(image) - Prepares an image for input to a neural network.

    This function applies transformations to the input image using the apply_transforms function, clones the resulting
    tensor, detaches it from the computation graph, sets requires_grad to True to enable gradient computation, and moves
    it to the specified device (CPU or CUDA).
    
    Parameters:
    image: The input image to be prepared for input to a neural network.
    
    Returns:
    torch.Tensor: The prepared image tensor ready for input to a neural network.
"""

def get_google_slide(url): #defines a function get_google_slide
    url_head = "https://docs.google.com/presentation/d/" #constructs a URL to export a Google Slides presentation as a PDF
    url_body = url.split('/')[5] #extracts the presentation ID from the provided URL
    page_id = url.split('.')[-1] #extracts the page ID from the provided URL
    return url_head + url_body + "/export/pdf?id=" + url_body + "&pageid=" + page_id #returns the URL

def get_slides(url): #defines a function get_slides
    url = get_google_slide(url) #gets the URL for PDF export
    r = requests.get(url, allow_redirects=True) #makes a GET request to the export URL
    open('file.pdf', 'wb').write(r.content) #saves the PDF content to a file
    images = convert_from_path('file.pdf', 500) #converts the PDF file to images at 500 DPI
    return images #returns the list of images

def load(image): #defines a function load

    return apply_transforms(image).clone().detach().requires_grad_(True).to(device) #applies transformations to an image and prepares it for input to a neural network

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #sets the device to CUDA if available, else CPU

labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()} #create a dictionary mapping integer class indices to readable class names by fetching the JSON file from the given URL and converting the string keys to integers

model = alexnet(weights='DEFAULT').to(device) #load the pre-trained AlexNet model from torchvision's model zoo with default weights, move it to the specified device (GPU or CPU)
model.eval(); #switch the model to evaluation mode, which disables layers like dropout and batch normalization, making it ready for inference

url = "https://docs.google.com/presentation/d/1P4j2N6uSwv88M_8yurO5d0tiWuL-DrlU0HVwh6cK1kg/edit#slide=id.p" #URL to Data Deck Google Slides Presentation (from Lab 8)

images = [] #initialize an empty list to store processed images

for image in get_slides(url): #iterate over each image obtained from the get_slides function

    plot(image) #display the image using the plot function defined earlier

    images.append(load(image)) #apply the load function to the image and append the processed image to the list

images = torch.vstack(images) #stack all the processed images into a single tensor

images.shape #check the shape of the images tensor, expecting a batch size of 50 with 3 color channels and 224x224 dimensions

model(images) #pass the images tensor through the model to get predictions (output is a tensor of logits)

y = model(images) #store the model's predictions for images in the variable y

y.shape #check the shape of the y tensor to confirm it matches the expected number of predictions (50 predictions, each with 1000 classes for ImageNet)

guesses = torch.argmax(y, 1).cpu().numpy() #get the index of the highest logit value in each prediction to find the predicted class, move the tensor to CPU, and convert it to a numpy array

for i in list(guesses): #iterate through the predicted class indices in guesses
    print(labels[i]) #print the readable label for each prediction from the labels dictionary

Y = np.zeros(50,) #create a numpy array Y of length 50 initialized with zeros
Y[25:] = 1 #set the second half of the array Y to ones

Y #display the array y to check its contents (the first half of the array has zeros and the second half has ones)

X = y.detach().cpu().numpy() #assuming y is a PyTorch tensor, detach it from the computation graph, move it to CPU, and convert it to a numpy array X

X.shape #check the shape of the array X, which is expected to be (50,)

plt.plot(X[0],'.') #plot the first element of array X as a scatter plot with dot markers

X[0] #access the first element of the array or tensor X

np.argmax(X[0]) #get the index of the maximum value in the first array of X

labels[948] #access the label corresponding to the class index 948 in the labels dictionary

top_ten = np.argsort(X[0])[::-1][0:10] #correct the slicing to get the indices of the top ten scores

for i in top_ten: #iterate over the top_ten indices
    print(labels[i]) #print the corresponding labels from the labels dictionary

labels #displays the entire labels dictionary

plt.hist(X[0]) #plots a histogram of the values in the first element of the array X

X = GPU_data(X) #transfer the input data X to the device (GPU/CPU) for computation
Y = GPU_data(Y) #transfer the input data Y to the device (GPU/CPU) for computation

"""Documentation Block: softmax(x) - Apply the softmax function to an input tensor x.

    The softmax function maps ℝ^n to the (n-1)-simplex and is used to convert a vector of raw scores (logits) into
    a distribution of probabilities that sum to 1. This implementation includes a numerical stability trick by
    subtracting the maximum value in each row before exponentiation to prevent overflow or underflow.

    Parameters:
    x (torch.Tensor): A tensor of logits to apply softmax to, often the output of a network's last linear layer.
    
    Returns:
    torch.Tensor: The resulting tensor after applying the softmax function, representing a probability distribution.
"""

def softmax(x): #apply the softmax function to an input tensor x
    s1 = torch.exp(x - torch.max(x,1)[0][:,None]) #subtract the max for numerical stability, and exponentiate x
    s = s1 / s1.sum(1)[:,None] #normalize the exponentiated values so that they sum up to 1 over the second dimension
    return s #outputs s

"""Documentation Block: cross_entropy(outputs, labels) - Calculate the cross-entropy loss between the predicted outputs and the true labels.

    Cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1.
    Cross-entropy loss increases as the predicted probability diverges from the actual label, with a perfect model having
    a cross-entropy loss of 0. This function applies softmax to the outputs to get the probabilities, then computes the
    negative log likelihood of the correct classes, and averages the result over all examples in the batch.
    
    Parameters:
    outputs (torch.Tensor): The logits as predicted by the model. The outputs before applying the softmax.
    labels (torch.Tensor): The true labels corresponding to the inputs of the model. Should be class indices.
    
    Returns:
    torch.Tensor: The average cross-entropy loss for the batch.
"""

def cross_entropy(outputs, labels): #calculate the cross-entropy loss given the outputs from the network and the target labels
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0] #uses the negative log likelihood of the softmax of the outputs, averaged over all examples

"""Documentation Block: Truncated_Normal(size) -  Generates random values from a truncated normal distribution given a size.
    
    This function uses the Box-Muller transform, which allows for the generation of standard normally distributed random
    numbers from uniformly distributed random numbers. The normal distribution is truncated at 2 standard deviations to
    limit the range of generated values. This can be useful when initializing weights in a neural network to avoid very
    large values that could adversely affect training.
    
    Parameters:
    size (tuple of ints): The dimensions of the tensor to be generated.
    
    Returns:
    torch.Tensor: A tensor containing random values from a truncated normal distribution
"""

def Truncated_Normal(size): #generates random numbers from a truncated normal distribution given a size

    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2) #generate a tensor of random values from a uniform distribution, scaled to the range of the standard normal distribution truncated at 2 standard deviations
    u2 = torch.rand(size) #generate a second tensor of random values from a standard uniform distribution
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2) #apply the Box-Muller transform to convert uniform random values to a normal distribution, which is then truncated

    return z #return the tensor of truncated normal random values

"""Documentation Block: acc(out,y) - Calculates the accuracy of the predictions out against the true labels y.
    
    Accuracy is computed as the ratio of correct predictions to the total number of samples. The function
    assumes that out contains raw scores or logits which are converted to predicted class indices through the argmax function,
    and compares these to the true labels. This is primarily used for evaluating classification models.
    
    Parameters:
    out (torch.Tensor): The logits predicted by the model as a 2D tensor where each row corresponds to a sample.
    y (torch.Tensor): The true labels as a 1D tensor of class indices.
    
    Returns:
    float: The computed accuracy as a float value between 0 and 1
"""

def acc(out,y): #calculate the accuracy of the predictions out against the true labels y
    with torch.no_grad(): #disable gradient calculation as it is not needed for evaluation
        return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0] #compute the number of correct predictions and divide by the total number of predictions to get the accuracy

X.shape #check the shape of the data tensor X to ensure it matches the expected structure for input into the model

"""Documentation Block: get_batch(mode) -  Fetches a batch of data for training or testing.

    This function retrieves a batch of input data and corresponding labels based on the specified mode (train or test).
    For the training mode, it randomly selects a starting index within the training data and slices the input data and
    labels to create a batch of the specified size. For the testing mode, it performs the same operation on the testing data.
    
    Parameters:
    mode (str): The mode indicating whether to fetch a batch for training (train) or testing (test).
    
    Returns:
    tuple: A tuple containing the batch of input data (x) and corresponding labels (y)
"""

def get_batch(mode): #creates a function get_batch (fetches a batch of data for training or testing)
    b = c.b #define the batch size
    if mode == "train": #checks if the mode is train
        r = np.random.randint(X.shape[0]-b) #randomly select a starting index r for a batch from the training data
        x = X[r:r+b,:] #slice the X array to create a batch of input data of size b
        y = Y[r:r+b] #slice the Y array to get the corresponding labels for the batch
    elif mode == "test": #checks if the mode is test
        r = np.random.randint(X_test.shape[0]-b) #randomly select a starting index r for a batch from the testing data
        x = X_test[r:r+b,:] #slice the X_test array to create a batch of input data of size b
        y = Y_test[r:r+b] #slice the Y_test array to get the corresponding labels for the batch
    return x,y #returns the batch of inputs data x and corresponding labels y

"""Documentation Block: model(x,w) - Defines a linear prediction model by computing the dot product of inputs and weights.

    This model is an example of linear regression without an intercept. It is used primarily for demonstrating
    basic neural network operations like matrix multiplications.

    Parameters:
    x (torch.Tensor): The input data, a batch of examples.
    w (torch.Tensor): The weights of the model, typically learned during training.

    Returns:
    torch.Tensor: The predicted outputs as the dot product of input data and weights.
"""

def model(x,w): #defines a function model

    return x@w[0] #returns the dot product of input x with weights w

"""Documentation Block: make_plots() - Generate and display plots for model training metrics.

    This function could plot various metrics such as training accuracy over epochs. This is typically used to plot libraries using
    data logged during the training process.

    Parameters:
    None

    Returns:
    None: Displays plots of training metrics
"""

def make_plots(): #defines a function make_plots

    acc_train = acc(model(x,w),y) #calculate the training accuracy using the 'acc' function and the 'model'

    wb.log({"acc_train": acc_train}) #log the training accuracy using Weights & Biases

wb.init(project="Linear_Model_Photo_1"); #initialize Weights & Biases for experiment tracking, with the project name "Linear_Model_Photo_1"
c = wb.config #set up configuration in Weights & Biases
#configure the training parameters and initialize the weights and biases tracking
c.h = 0.001 #learning rate for the optimizer
c.b = 4 #batch size for training
c.epochs = 100000 #total number of training epochs

w = [GPU(Truncated_Normal((1000,2)))] #initialize the weights of the model using a truncated normal distribution and transfer to GPU

optimizer = torch.optim.Adam(w, lr=c.h) #set up the optimizer to use with the model (i.e. Adam, with the learning rate from the configuration)

for i in range(c.epochs): #training loop over the number of epochs specified in the configuration

    x,y = get_batch('train') #fetch a batch of training data

    loss = cross_entropy(softmax(model(x,w)),y) #compute the loss using cross-entropy between the model's predictions and the true labels
    #clear previous gradients, compute gradients of the loss, and perform a single optimization step
    optimizer.zero_grad() #reset gradients to zero before backward pass
    loss.backward() #compute the gradient of the loss with respect to model parameters
    optimizer.step() #update model parameters

    wb.log({"loss": loss}) #log the current loss to Weights & Biases

    make_plots() #call function to create plots (details not provided in the snippet)

"""Loss Chart(s) - https://api.wandb.ai/links/akayne/lbjddfz1, https://api.wandb.ai/links/akayne/s11agwg6


Acc_train Chart(s) - https://api.wandb.ai/links/akayne/o696fkfa,  https://api.wandb.ai/links/akayne/w3ww3be3

(Colab did not allow me to upload as a PDF or PNG image, so here are the links instead)

The loss chart indicates an unstable training process, with prediction error fluctuating drastically rather than smoothly decreasing. The training accuracy chart shows highly variable performance just like the loss chart, with accuracy oscillating sharply instead of steadily increasing. Thus, both of these charts are inconsistent with signifies that the training process may need to be adjusted. If it is adjusted then the training accuracy can steadily increase and loss can consistently decrease which would improve predictions.
"""





