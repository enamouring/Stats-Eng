# -*- coding: utf-8 -*-
"""Audrey_Analog_v_Digital_Clock_Alexnet Load Slides All Hahn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19pvbFDQ00AgYF-l_Bg6BJKm47H0CTKoH
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install wandb #installs weight and bias library
# !apt-get install poppler-utils #install utilities for PDF files
# !pip install pdf2image #installs pdf to images library (convert)
# !pip install flashtorch #install PyTorch-based visualizations
# import requests #to make HTTP requests
# from pdf2image import convert_from_path #to convert PDF files to images
# import matplotlib.pyplot as plt #to generate plots
# import numpy as np #for numerical operations
# import torch #for tensor computations
# import requests
# from torchvision import * #for computer vision
# from torchvision.models import * #pretained models in torchvision
# import wandb as wb #weight and bias libary

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #sets device to GPU (if available) else sets it to CPU

def GPU(data): #defining a function that moves data to GPU with gradient computation enabled
    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=device)

def GPU_data(data): #defining a function that moves data to GPU without gradient computation enabled
    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=device)

def plot(x): #defining a function to plot an image
    fig, ax = plt.subplots()
    im = ax.imshow(x, cmap = 'gray')
    ax.axis('off')
    fig.set_size_inches(5, 5)
    plt.show()

def get_google_slide(url): #defining a function that generates URL and exports a google slide as PDF
    url_head = "https://docs.google.com/presentation/d/"
    url_body = url.split('/')[5] #gets the presentation ID
    page_id = url.split('.')[-1] #gets the page ID
    return url_head + url_body + "/export/pdf?id=" + url_body + "&pageid=" + page_id #makes the URL for the PDF

def get_slides(url): #allows us to fetch slides and convert them to images
    url = get_google_slide(url)
    r = requests.get(url, allow_redirects=True) #fetch PDF
    open('file.pdf', 'wb').write(r.content) #save PDF file locally
    images = convert_from_path('file.pdf', 500) #convert PDF to images
    return images

def load(image, size=224): #cropping images; loads and preprocesses images for model input
    means = [0.485, 0.456, 0.406] #image normalization mean values
    stds = [0.229, 0.224, 0.225] #image normalization standard deviation values
    transform = transforms.Compose([
        transforms.Resize(size), #resizes image
        transforms.CenterCrop(size), #center crop image
        transforms.ToTensor(),  #convert the image to PyTorch tensor
        transforms.Normalize(means, stds)
    ])
    tensor = transform(image).unsqueeze(0).to(device)  #apply transformation and move tensor to device
    tensor.requires_grad = True #enable gradient computation for tensor
    return tensor

labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()} #fetches labels from the url

model = alexnet(weights='DEFAULT').to(device) #loads the AlexNet model and move to device
model.eval(); #set model to evaluation mode

url = "https://docs.google.com/presentation/d/1P4j2N6uSwv88M_8yurO5d0tiWuL-DrlU0HVwh6cK1kg/edit#slide=id.p" #slides link to take from

images = [] #creates a stack of images from the slide url provided

for image in get_slides(url): #for all images in the url

    plot(image) #plots the image

    images.append(load(image)) #appends each image onto the other (like adding strings)

images = torch.vstack(images) #final result is stored in images

images.shape #shows the dimensions of images

model(images) #passes the images through the model to get predictions

y = model(images) #defines y to be model(images)

y.shape  #takes the shape of the mdoerl 50x1000

guesses = torch.argmax(y, 1).cpu().numpy() #index of max value

for i in list(guesses): #iterate for total guesses in the list
    print(labels[i]) #prints all possible labels for the images

Y = np.zeros(50,) #initalize a np array with 50 zeros
Y[25:] = 1 #sets all values after 25 to be 1

Y #displays y

# Y = np.zeros(100,) #(in this case we would have 100 zeros, and after 50 they would be 1s)
# Y[50:] = 1

Y

X = y.detach().cpu().numpy() #convert the predictions tensor to a numpy array and detach it from the computation graph

X.shape #50x1000

plt.plot(X[0],'.') #plots the first array row

plt.hist(X[0]) #plot the first array row (histogram)

X = GPU_data(X) #moves data to GPU for processing
Y = GPU_data(Y) #moves the modified data array to GPU for processing

def softmax(x):
    s1 = torch.exp(x - torch.max(x,1)[0][:,None])
    s = s1 / s1.sum(1)[:,None]
    return s

def cross_entropy(outputs, labels): #cross-entropy loss function; computes the cross-entropy loss between predicted outputs and true labels
    return -torch.sum(softmax(outputs).log()[range(outputs.size()[0]), labels.long()])/outputs.size()[0]

def randn_trunc(s): #Truncated Normal Random Numbers
    mu = 0
    sigma = 0.1
    R = stats.truncnorm((-2*sigma - mu) / sigma, (2*sigma - mu) / sigma, loc=mu, scale=sigma)
    return R.rvs(s)

def Truncated_Normal(size): #truncated normal random numbers

    u1 = torch.rand(size)*(1-np.exp(-2)) + np.exp(-2)
    u2 = torch.rand(size)
    z  = torch.sqrt(-2*torch.log(u1)) * torch.cos(2*np.pi*u2)

    return z #returns numbers with shape 'size' following the TN distribution

def acc(out,y): #accuracy function
    with torch.no_grad():
        return (torch.sum(torch.max(out,1)[1] == y).item())/y.shape[0]

X.shape #checks shape of X (50x1000)

def get_batch(mode): #batch of data
    b = c.b #batch size
    if mode == "train":
        r = np.random.randint(X.shape[0]-b) #random index for training batch
        x = X[r:r+b,:] #training batch input
        y = Y[r:r+b] #training batch output
    elif mode == "test":
        r = np.random.randint(X_test.shape[0]-b) #random index for test batch
        x = X_test[r:r+b,:] #test batch input
        y = Y_test[r:r+b] #test batch output
    return x,y

def model(x,w): #linear model

    return x@w[0] #matrix mult to return model predictions

def make_plots(): #make plots and log training accuracy

    acc_train = acc(model(x,w),y) #define training acc

    # xt,yt = get_batch('test')

    # acc_test = acc(model(xt,w),yt)

    wb.log({"acc_train": acc_train}) #log training acc

wb.init(project="Linear_Model_Photo_1"); #weights and bias project
c = wb.config #configuration object

c.h = 0.001 #learning rate
c.b = 32 #batch size
c.epochs = 100000 #number of epochs

w = [GPU(Truncated_Normal((1000,2)))] #initialize weights with TN Random Numbers

optimizer = torch.optim.Adam(w, lr=c.h)

for i in range(c.epochs): #training loop

    x,y = get_batch('train') #to get training batch

    loss = cross_entropy(softmax(model(x,w)),y) #calculate loss

    optimizer.zero_grad()
    loss.backward() #back propagation
    optimizer.step()

    wb.log({"loss": loss}) #log the loss

    make_plots() #make plots and log accuracy





















